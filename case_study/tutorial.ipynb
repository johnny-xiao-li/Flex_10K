{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c743a7",
   "metadata": {},
   "source": [
    "# Predicting Volatility from Text Data: A Time Series Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24db09",
   "metadata": {},
   "source": [
    "This notebook demonstrates a time series cross-validation approach to predict future stock volatility based on text data (e.g., news headlines, articles). It utilizes TF-IDF for text feature extraction and XGBoost for the regression task. The methodology employs an expanding window approach, where the model is trained on an increasing historical dataset and evaluated on the subsequent year's data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b2aa36",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d0cf6",
   "metadata": {},
   "source": [
    "First, we import all the necessary Python libraries for data manipulation (`pandas`, `numpy`), text processing (`TfidfVectorizer` from `sklearn.feature_extraction.text`), machine learning model (`XGBRegressor` from `xgboost`), and evaluation metrics (`mean_squared_error` from `sklearn.metrics`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c29a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db01071",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d02f2c",
   "metadata": {},
   "source": [
    "In this section, we load the dataset, perform initial text preprocessing by converting the 'cleaned_text' column to lowercase, and extract the 'year' from the 'Date' column to facilitate time-based data splitting for the cross-validation. We then display the head of the processed DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41dd2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Symbol        Date    vol_3d    vol_7d   vol_15d   vol_30d   vol_60d  \\\n",
      "CIK                                                                            \n",
      "796343    ADBE  2021-01-15  0.254187  0.163928  0.267558  0.266536  0.295177   \n",
      "920760     LEN  2021-01-22  0.250670  0.222891  0.317954  0.450116  0.490832   \n",
      "1067701    URI  2021-01-27  0.897888  0.592564  0.453141  0.458637  0.402286   \n",
      "87347      SLB  2021-01-27  0.448659  0.459556  0.355023  0.415898  0.410442   \n",
      "1326801   META  2021-01-28  0.314609  0.227651  0.184727  0.307393  0.295126   \n",
      "\n",
      "          vol_90d                                       cleaned_text  year  \n",
      "CIK                                                                         \n",
      "796343   0.273107  as previously discussed, our actual results co...  2021  \n",
      "920760   0.448351  the following are what we believe to be the pr...  2021  \n",
      "1067701  0.363955  our business, results of operations and financ...  2021  \n",
      "87347    0.424772  the following discussion of risk factors known...  2021  \n",
      "1326801  0.299502  certain factors may have a material adverse ef...  2021  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the specified CSV file.\n",
    "# The 'index_col=0' argument tells pandas to use the first column as the DataFrame index.\n",
    "df_data = pd.read_csv('./Data/cleaned_dataset_2124.csv', index_col=0)\n",
    "\n",
    "# Convert the 'cleaned_text' column to lowercase.\n",
    "# This step standardizes the text, ensuring that words like \"Volatility\" and \"volatility\" are treated identically\n",
    "# during feature extraction, which helps improve model consistency.\n",
    "df_data['cleaned_text'] = df_data['cleaned_text'].str.lower()\n",
    "\n",
    "# Extract the year from the 'Date' column and create a new 'year' column.\n",
    "# The 'Date' column is first converted to datetime objects, and then the .dt.year accessor is used.\n",
    "# This 'year' column is essential for implementing the time series cross-validation strategy.\n",
    "df_data['year'] = pd.to_datetime(df_data['Date']).dt.year\n",
    "\n",
    "# Display the first 5 rows of the modified DataFrame.\n",
    "# This helps in quickly inspecting the data structure, confirming the lowercase conversion,\n",
    "# and verifying the newly added 'year' column.\n",
    "print(df_data.head(5))# Load the dataset from the specified CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ce0ed",
   "metadata": {},
   "source": [
    "## 3. Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68501c69",
   "metadata": {},
   "source": [
    "This step provides a statistical summary of the numerical columns in the DataFrame. This helps in understanding the distribution, central tendency, and spread of the volatility target variables (`vol_3d` to `vol_90d`) column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2af78e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vol_3d</th>\n",
       "      <th>vol_7d</th>\n",
       "      <th>vol_15d</th>\n",
       "      <th>vol_30d</th>\n",
       "      <th>vol_60d</th>\n",
       "      <th>vol_90d</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1729.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.180231</td>\n",
       "      <td>0.261484</td>\n",
       "      <td>0.285047</td>\n",
       "      <td>0.282210</td>\n",
       "      <td>0.297074</td>\n",
       "      <td>0.292340</td>\n",
       "      <td>2022.529786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.214962</td>\n",
       "      <td>0.177482</td>\n",
       "      <td>0.158320</td>\n",
       "      <td>0.134701</td>\n",
       "      <td>0.130181</td>\n",
       "      <td>0.121974</td>\n",
       "      <td>1.122352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.023844</td>\n",
       "      <td>0.034914</td>\n",
       "      <td>0.076416</td>\n",
       "      <td>0.084912</td>\n",
       "      <td>0.086059</td>\n",
       "      <td>2021.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.059327</td>\n",
       "      <td>0.155061</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>0.196017</td>\n",
       "      <td>0.211379</td>\n",
       "      <td>0.209553</td>\n",
       "      <td>2022.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.127947</td>\n",
       "      <td>0.225854</td>\n",
       "      <td>0.252115</td>\n",
       "      <td>0.251763</td>\n",
       "      <td>0.267466</td>\n",
       "      <td>0.264587</td>\n",
       "      <td>2023.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.236228</td>\n",
       "      <td>0.312807</td>\n",
       "      <td>0.340543</td>\n",
       "      <td>0.333483</td>\n",
       "      <td>0.349459</td>\n",
       "      <td>0.339083</td>\n",
       "      <td>2024.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.974497</td>\n",
       "      <td>3.743436</td>\n",
       "      <td>3.007637</td>\n",
       "      <td>2.186403</td>\n",
       "      <td>1.682482</td>\n",
       "      <td>1.379846</td>\n",
       "      <td>2024.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            vol_3d       vol_7d      vol_15d      vol_30d      vol_60d  \\\n",
       "count  1729.000000  1729.000000  1729.000000  1729.000000  1729.000000   \n",
       "mean      0.180231     0.261484     0.285047     0.282210     0.297074   \n",
       "std       0.214962     0.177482     0.158320     0.134701     0.130181   \n",
       "min       0.000008     0.023844     0.034914     0.076416     0.084912   \n",
       "25%       0.059327     0.155061     0.185311     0.196017     0.211379   \n",
       "50%       0.127947     0.225854     0.252115     0.251763     0.267466   \n",
       "75%       0.236228     0.312807     0.340543     0.333483     0.349459   \n",
       "max       4.974497     3.743436     3.007637     2.186403     1.682482   \n",
       "\n",
       "           vol_90d         year  \n",
       "count  1729.000000  1729.000000  \n",
       "mean      0.292340  2022.529786  \n",
       "std       0.121974     1.122352  \n",
       "min       0.086059  2021.000000  \n",
       "25%       0.209553  2022.000000  \n",
       "50%       0.264587  2023.000000  \n",
       "75%       0.339083  2024.000000  \n",
       "max       1.379846  2024.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6062db98",
   "metadata": {},
   "source": [
    "## 4. Time Series Cross-Validation and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c998ff3",
   "metadata": {},
   "source": [
    "* **Outer Loop:** Iterates through test years (2022, 2023, 2024). For each iteration, the training data includes all years *before* the current test year, and the testing data is the current year. This simulates how a model would be deployed and retrained over time.\n",
    "* **Inner Loop:** For each test year, the model's performance is evaluated for different prediction horizons (3, 7, 15, 30, 60, 90 days), represented by the `vol_Xd` columns.\n",
    "\n",
    "For each combination of test year and prediction horizon:\n",
    "1.  **Data Splitting:** Data is split into training and testing sets based on the year.\n",
    "2.  **TF-IDF Vectorization:** Text data is converted into numerical features using `TfidfVectorizer`. The vectorizer is `fit_transform` on the training data and `transform` on the test data to prevent data leakage.\n",
    "3.  **XGBoost Model Training:** An `XGBRegressor` model is trained on the vectorized training data and the corresponding volatility target.\n",
    "4.  **Prediction and Evaluation:** Predictions are made on the test set, and the Mean Squared Error (MSE) is calculated and printed. All MSE results are collected in a list for final display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7ef2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on data from 2021 to 2021 and testing on 2022\n",
      "Number of training samples: 419\n",
      "Number of testing samples: 429\n",
      "Training model for 3 days ahead prediction\n",
      "Mean Squared Error: 0.10674581435768475\n",
      "---------------\n",
      "Training model for 7 days ahead prediction\n",
      "Mean Squared Error: 0.05721633585441949\n",
      "---------------\n",
      "Training model for 15 days ahead prediction\n",
      "Mean Squared Error: 0.038864257855644804\n",
      "---------------\n",
      "Training model for 30 days ahead prediction\n",
      "Mean Squared Error: 0.02109916411113449\n",
      "---------------\n",
      "Training model for 60 days ahead prediction\n",
      "Mean Squared Error: 0.021704831856504167\n",
      "---------------\n",
      "Training model for 90 days ahead prediction\n",
      "Mean Squared Error: 0.02285179703077452\n",
      "---------------\n",
      "Training on data from 2021 to 2022 and testing on 2023\n",
      "Number of training samples: 848\n",
      "Number of testing samples: 427\n",
      "Training model for 3 days ahead prediction\n",
      "Mean Squared Error: 0.06578927614347502\n",
      "---------------\n",
      "Training model for 7 days ahead prediction\n",
      "Mean Squared Error: 0.046809124591112646\n",
      "---------------\n",
      "Training model for 15 days ahead prediction\n",
      "Mean Squared Error: 0.02884009423335826\n",
      "---------------\n",
      "Training model for 30 days ahead prediction\n",
      "Mean Squared Error: 0.015329526842526807\n",
      "---------------\n",
      "Training model for 60 days ahead prediction\n",
      "Mean Squared Error: 0.011638605660552165\n",
      "---------------\n",
      "Training model for 90 days ahead prediction\n",
      "Mean Squared Error: 0.008444637788288517\n",
      "---------------\n",
      "Training on data from 2021 to 2023 and testing on 2024\n",
      "Number of training samples: 1275\n",
      "Number of testing samples: 454\n",
      "Training model for 3 days ahead prediction\n",
      "Mean Squared Error: 0.03186938841823621\n",
      "---------------\n",
      "Training model for 7 days ahead prediction\n",
      "Mean Squared Error: 0.017504996692732075\n",
      "---------------\n",
      "Training model for 15 days ahead prediction\n",
      "Mean Squared Error: 0.014947948082930393\n",
      "---------------\n",
      "Training model for 30 days ahead prediction\n",
      "Mean Squared Error: 0.011968922622146619\n",
      "---------------\n",
      "Training model for 60 days ahead prediction\n",
      "Mean Squared Error: 0.012211034086178716\n",
      "---------------\n",
      "Training model for 90 days ahead prediction\n",
      "Mean Squared Error: 0.009265169328453763\n",
      "---------------\n",
      "[{'year': 2022, 'mse': [0.10674581435768475, 0.05721633585441949, 0.038864257855644804, 0.02109916411113449, 0.021704831856504167, 0.02285179703077452]}, {'year': 2023, 'mse': [0.06578927614347502, 0.046809124591112646, 0.02884009423335826, 0.015329526842526807, 0.011638605660552165, 0.008444637788288517]}, {'year': 2024, 'mse': [0.03186938841823621, 0.017504996692732075, 0.014947948082930393, 0.011968922622146619, 0.012211034086178716, 0.009265169328453763]}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store evaluation metrics for each test year.\n",
    "# Each element will be a dictionary containing the year and a list of MSE scores\n",
    "# for different prediction horizons within that year.\n",
    "evaluation_year = []\n",
    "\n",
    "# Outer loop: Iterate through the test years from 2022 to 2024 (exclusive of 2025).\n",
    "# This loop simulates an expanding window approach for time series cross-validation.\n",
    "for i in range(2022, 2025):\n",
    "    # Print the current training and testing year range for clarity.\n",
    "    print(f\"Training on data from 2021 to {i-1} and testing on {i}\")\n",
    "\n",
    "    # Split the dataset into training and testing sets based on the current year 'i'.\n",
    "    # df_data_train: Contains all rows where the 'year' is strictly less than 'i'.\n",
    "    df_data_train = df_data[df_data['year'] < i]\n",
    "    # df_data_test: Contains all rows where the 'year' is exactly 'i'.\n",
    "    df_data_test = df_data[df_data['year'] == i]\n",
    "\n",
    "    # Print the number of samples in the training and testing sets.\n",
    "    # This helps verify the correctness of the data split for each fold.\n",
    "    print(f\"Number of training samples: {df_data_train.shape[0]}\")\n",
    "    print(f\"Number of testing samples: {df_data_test.shape[0]}\")\n",
    "\n",
    "    # Initialize a list to store Mean Squared Error (MSE) scores for different prediction horizons\n",
    "    # within the current test year.\n",
    "    evaluation_ndays_mse = []\n",
    "\n",
    "    # Inner loop: Iterate through different prediction horizons (in days).\n",
    "    # These correspond to the 'vol_Xd' target columns in the dataset.\n",
    "    for n_days in [3, 7, 15, 30, 60, 90]:\n",
    "        # Print the current prediction horizon being processed.\n",
    "        print(f\"Training model for {n_days} days ahead prediction\")\n",
    "\n",
    "        # Dynamically construct the target column name based on the current 'n_days'.\n",
    "        target_col = f'vol_{n_days}d'\n",
    "\n",
    "        # Define the feature (X) and target (y) for the training set.\n",
    "        X_train = df_data_train['cleaned_text']\n",
    "        y_train = df_data_train[target_col]\n",
    "\n",
    "        # Define the feature (X) and target (y) for the testing set.\n",
    "        X_test = df_data_test['cleaned_text']\n",
    "        y_test = df_data_test[target_col]\n",
    "\n",
    "        # Initialize TfidfVectorizer.\n",
    "        # max_features=5000: Limits the vocabulary size to the top 5000 most frequent terms,\n",
    "        # which helps manage dimensionality and reduce noise.\n",
    "        # ngram_range=(1, 2): Considers both unigrams (single words) and bigrams (two-word phrases)\n",
    "        # as features, capturing more context from the text.\n",
    "        vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "        # Fit the vectorizer on the training text data and transform it into TF-IDF features.\n",
    "        # This step learns the vocabulary and IDF weights from the training data.\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "        # Transform the testing text data using the *same* fitted vectorizer.\n",
    "        # It's crucial to only 'transform' the test data to prevent data leakage from the test set\n",
    "        # into the feature engineering process.\n",
    "        X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "        # Initialize and train the XGBoost Regressor model.\n",
    "        # Hyperparameters are set as specified in the original code:\n",
    "        # n_estimators=1000: Number of boosting rounds (i.e., number of trees to build).\n",
    "        # learning_rate=0.01: Step size shrinkage used in updates to prevent overfitting.\n",
    "        # max_depth=6: Maximum depth of a tree.\n",
    "        # min_child_weight=1: Minimum sum of instance weight (hessian) needed in a child.\n",
    "        # subsample=0.8: Fraction of samples used for fitting the trees.\n",
    "        # colsample_bytree=0.8: Fraction of features (columns) used when constructing each tree.\n",
    "        # gamma=0: Minimum loss reduction required to make a further partition on a leaf node.\n",
    "        # reg_alpha=0.1 (L1 regularization): Regularization term on weights.\n",
    "        # reg_lambda=1 (L2 regularization): Regularization term on weights.\n",
    "        xgb_model = XGBRegressor(n_estimators=1000,\n",
    "                                 learning_rate=0.01,\n",
    "                                 max_depth=6,\n",
    "                                 min_child_weight=1,\n",
    "                                 subsample=0.8,\n",
    "                                 colsample_bytree=0.8,\n",
    "                                 gamma=0,\n",
    "                                 reg_alpha=0.1,\n",
    "                                 reg_lambda=1\n",
    "                                )\n",
    "        # Train the XGBoost model using the TF-IDF features and the corresponding volatility targets.\n",
    "        xgb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "        # Make predictions on the TF-IDF vectorized test data.\n",
    "        y_pred = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "        # Calculate Mean Squared Error (MSE) between the actual test volatility (y_test)\n",
    "        # and the predicted volatility (y_pred).\n",
    "        # MSE is a common metric for regression tasks, measuring the average squared difference.\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        # Print the calculated MSE for the current prediction horizon.\n",
    "        print(f'Mean Squared Error: {mse}')\n",
    "        print('---------------') # Separator for readability in output\n",
    "\n",
    "        # Append the calculated MSE score to the list for the current year.\n",
    "        evaluation_ndays_mse.append(mse)\n",
    "\n",
    "    # After iterating through all prediction horizons for the current test year,\n",
    "    # append the collected MSE scores for that year to the main 'evaluation_year' list.\n",
    "    evaluation_year.append({\n",
    "        'year': i,\n",
    "        'mse': evaluation_ndays_mse\n",
    "    })\n",
    "\n",
    "# Print the final aggregated evaluation results.\n",
    "# This list contains dictionaries, each summarizing the MSE scores for all prediction horizons\n",
    "# across each of the test years.\n",
    "print(evaluation_year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
